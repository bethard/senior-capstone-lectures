\documentclass{beamer}
\input{../common-definitions.tex}

\title[Ethical Codes]{Ethical Codes}
\date{August~$29^{\text{th}}$,~2013}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Quiz}

\begin{block}{1. A utilitarian ethical code is...}
\begin{enumerate}[(A)]
\item<1-3> $\displaystyle \argmax_{a \in actions} \left( \sum_{h \in humans} \left( happiness(h, a) - unhappiness(h, a)\right)\right)$
\item<1> $\displaystyle \argmax_{a \in actions} \left( \sum_{h \in humans} happiness(h, a)\right)$
\item<1> $\displaystyle \argmax_{a \in actions} \left( \sum_{h \in humans} \left( unhappiness(h, a) - happiness(h, a)\right)\right)$
\end{enumerate}
\end{block}
\begin{block}{2. Kant believed...}
\begin{enumerate}[(A)]
\item<1-2> We have a direct duty to avoid torturing animals
\item<1-2> Humans may be used as a means to an end
\item<1-3> Criminal punishment is a form of respect for the criminal
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Utilitarianism}
Actions judged by impact on happiness of everyone
\begin{itemize}
\item Intent of action is irrelevant
\item Only consequences matter
\end{itemize}
\bigskip
Summary:
\begin{itemize}
\item $\displaystyle \argmax_{a \in actions} \left( \sum_{h \in humans} \left( happiness(h, a) - unhappiness(h, a)\right)\right)$
\end{itemize}
\end{frame}

\begin{frame}{Utilitarianism: History}
Jeremy Bentham, 1789
\begin{itemize}
\item \textit{Nature has placed mankind under the governance of two sovereign masters, pain and pleasure\ldots
By the principle of utility is meant that principle which approves or disapproves of every action whatsoever according to the tendency it appears to have to augment or diminish the happiness of the party whose interest is in question\ldots}
\item Pleasure/pain measured by: intensity, duration, certainty, propinquity, fecundity, purity, extent
\end{itemize}
\end{frame}

\begin{frame}{Utilitarianism: History}
John Stuart Mill, 1861
\begin{itemize}
\item On why happiness = good: \textit{\ldots we have not only all the proof which the case admits of, but all which it is possible to require, that happiness is a good: that each person's happiness is a good to that person, and the general happiness, therefore, a good to the aggregate of all persons.}
\end{itemize}
G. E. Moore, 1912
\begin{itemize}
\item On why happiness $\neq$ good: \textit{It involves our saying that, even if the total quantity of pleasure in each [world] was exactly equal, yet the fact that all the beings in the one possessed in addition knowledge of many different kinds and a full appreciation of all that was beautiful or worthy of love in their world, whereas none of the beings in the other possessed any of these things, would give us no reason whatever for preferring the former to the latter.}
\end{itemize}
\end{frame}

\begin{frame}{Example: Build a new Highway stretch}
Scenario:
  \begin{itemize}
  \item State may replace a curvy stretch of highway
  \item New highway segment 1 mile shorter (benefit)
  \item 150 houses would have to be removed (cost)
  \item Some wildlife habitat would be destroyed (cost)
  \end{itemize}
\pause
Analysis:
  \begin{itemize}
  \item[-] \$20 million to compensate homeowners
  \item[-] \$10 million to construct new highway
  \item[-] Lost wildlife habitat worth \$1 million
  \item[+] \$39 million savings in automobile driving costs
  \end{itemize}
Conclusion:
\begin{itemize}
\item \$39 million > \$31 million $\Rightarrow$ Building highway is a good action
\end{itemize}
\end{frame}

\begin{frame}{Evaluating Utilitarianism}
\begin{itemize}
  \item[+] Considers all stakeholders
  \item[+] Emphasizes the greatest good (i.e., happiness)
  \item[+] Cost/benefit analysis is a well-understood tool
  \item[+] Understandable ethical theory
\pause
  \item[-] Must be able to exactly quantify ``good''
  \item[-] Must be able to accurately predict all consequences
  \item[-] Majority outnumbers minority
  \item[-] Every situation has to be individually analyzed
\end{itemize}
\end{frame}

\begin{frame}{Categorical imperative}
Defined by Immanuel Kant (1724-1804)
\begin{itemize}
\item An (un)ethical action is always (un)ethical
\item Expected consequences of an action are morally neutral
\end{itemize}
\pause
\medskip
Formulation 1: \textit{Act only according to that maxim whereby you can, at the same time, will that it should become a universal law.}
\begin{itemize}
\item $\stackrel{?}{\approx}$ Golden Rule (\textit{Treat others how you wish to be treated})
\item Key difference: not just \emph{you}, but \emph{everyone}
\end{itemize}
\pause
\medskip
Formulation2: \textit{Act in such a way that you treat humanity, whether in your own person or in the person of any other, nevermerely as a means to an end, but always at the same time as an end.}
\begin{itemize}
\item The end does not justify the means
\item Must allow others to make their own decisions
\end{itemize}
\end{frame}

\begin{frame}{Categorical imperative: Examples}
Stealing is unethical
\begin{itemize}
\item If everyone stole, property (and thus stealing) is meaningless
\item Stealing does not allow the owner to \emph{choose} to give item away
\end{itemize}
\medskip
\pause
Charity is ethical
\begin{itemize}
\item If no one ever helps others, no one can get help themselves
\item (Assumes that everyone needs help some time)
\end{itemize}
\medskip
\pause
Lying (or any deception) is unethical
\begin{itemize}
\item If everyone always lied, language is meaningless
\item Lying does not allow the lied-to to make their own decision
\end{itemize}
\pause
\medskip
\textbf{But should you tell a murderer where their victim is?}
\end{frame}

\begin{frame}{Categorical Imperative: Evaluation}
\begin{itemize}
\item[+] Universal maxims; easily applied to many situations
\item[+] Maxims are derived purely through reasoning
\item[+] Treats all persons as moral equals
\item[-] There are no exceptions to moral laws
\item[-] How do we resolve conflicts between rules?
\item[-] What are the underlying fundamental morals?
\end{itemize}
\end{frame}

\begin{frame}{Ethical Dilemma: Driverless Cars}
In three states -- Nevada, Florida, and California -- it is now legal for Google to operate its driverless cars. Is it ethical to put driverless cars on the road? If not, what (if any) conditions would be necessary to make it ethical?
\bigskip
\begin{itemize}
\item Under utilitarianism?
\item Under the categorical imperative?
\end{itemize}
\end{frame}

\begin{frame}{Ethical Dilemma: Using wi-fi}
Your neighbor's wi-fi is not password protected. Is it ethical for you to use it? If not, what (if any) conditions would be necessary to make it ethical?
\bigskip
\begin{itemize}
\item Under utilitarianism?
\item Under the categorical imperative?
\end{itemize}
\end{frame}

\begin{frame}{Ethical Dilemma: Genetically Modified Crops}
A genetically modified strain of corn is more nutritious and more drought, stress and insect resistant than traditional corn. However, the patent for this strain is owned by a single company. Is it ethical to replace all our traditional corn crops with this genetically modified corn? If not, what (if any) conditions would be necessary to make it ethical?
\bigskip
\begin{itemize}
\item Under utilitarianism?
\item Under the categorical imperative?
\end{itemize}
\end{frame}
\end{document}
